{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWfartrSH6A0",
        "outputId": "d18a9e0e-b7e4-4b9e-d94f-e895e292421c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:09:35_Pacific_Daylight_Time_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R39wi9gnIAHq",
        "outputId": "e98a6c18-361c-46f3-f9eb-038935e98419"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RkcP115IJbG",
        "outputId": "45bd6b2e-271b-4735-8cc9-0ec2b3320985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source files will be saved in \"C:\\Users\\KXIF\\AppData\\Local\\Temp\\tmpwxhby7u3\".\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkXM87mIza_"
      },
      "source": [
        "# **1) Hello World**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVLPm4veIPer",
        "outputId": "60e12f79-ee04-4e3e-bd91-04dc0e644491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, CUDA!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include<stdio.h>\n",
        "\n",
        "__global__ void helloCUDA() {\n",
        "    printf(\"Hello, CUDA!\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    helloCUDA<<<1, 1>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBz4KEk3JJ5s"
      },
      "source": [
        "# **2) Vector Addition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc1ByHw5Ivwk",
        "outputId": "c700d29a-6068-49d9-b9c4-6866ac248a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 + 0 = 0\n",
            "1 + 2 = 3\n",
            "2 + 4 = 6\n",
            "3 + 6 = 9\n",
            "4 + 8 = 12\n",
            "5 + 10 = 15\n",
            "6 + 12 = 18\n",
            "7 + 14 = 21\n",
            "8 + 16 = 24\n",
            "9 + 18 = 27\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 10 // represents the size of the vectors\n",
        "\n",
        "__global__ void vectorAdd(int *a, int *b, int *c) {\n",
        "    int tid = threadIdx.x;\n",
        "    if (tid < N) {\n",
        "        c[tid] = a[tid] + b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int a[N], b[N], c[N];\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    cudaMalloc((void**)&d_a, N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_b, N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_c, N * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        a[i] = i;\n",
        "        b[i] = 2 * i;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(d_a, a, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    vectorAdd<<<1, N>>>(d_a, d_b, d_c);\n",
        "\n",
        "    cudaMemcpy(c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
        "    }\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt2ytUPmJ5JY"
      },
      "source": [
        "# **3) Matrix Multiplication**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i06FRBJJzku",
        "outputId": "59a027c9-4c27-435b-c1aa-220e201089be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\t36\t42\t\n",
            "84\t108\t132\t\n",
            "138\t180\t222\t\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 3\n",
        "\n",
        "__global__ void matrixMul(int *a, int *b, int *c) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int sum = 0;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        for (int k = 0; k < N; k++) {\n",
        "            sum += a[row * N + k] * b[k * N + col];\n",
        "        }\n",
        "        c[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&d_a, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_b, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_c, N * N * sizeof(int));\n",
        "\n",
        "    // Initialize host matrices\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        a[i / N][i % N] = i;\n",
        "        b[i / N][i % N] = 2 * i;\n",
        "    }\n",
        "\n",
        "    // Copy host matrices to device\n",
        "    cudaMemcpy(d_a, a, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Set up grid and block dimensions\n",
        "    dim3 gridDim(1, 1, 1);\n",
        "    dim3 blockDim(N, N, 1);\n",
        "\n",
        "    // Launch the matrixMul kernel with specified grid and block dimensions\n",
        "    matrixMul<<<gridDim, blockDim>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(c, d_c, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print result\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            printf(\"%d\\t\", c[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2P4vl4LKfzL"
      },
      "source": [
        "# **4) Matrix Addition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jfAqGuEKIRR",
        "outputId": "5eb96c0d-cf56-4254-a3de-1ee0f35b902e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix A:\n",
            "0\t1\t2\t\n",
            "3\t4\t5\t\n",
            "6\t7\t8\t\n",
            "\n",
            "Matrix B:\n",
            "0\t2\t4\t\n",
            "6\t8\t10\t\n",
            "12\t14\t16\t\n",
            "\n",
            "Matrix C (A + B):\n",
            "0\t3\t6\t\n",
            "9\t12\t15\t\n",
            "18\t21\t24\t\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 3\n",
        "\n",
        "__global__ void matrixAdd(int *a, int *b, int *c) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        c[row * N + col] = a[row * N + col] + b[row * N + col];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&d_a, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_b, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_c, N * N * sizeof(int));\n",
        "\n",
        "    // Initialize host matrices\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        a[i / N][i % N] = i;\n",
        "        b[i / N][i % N] = 2 * i;\n",
        "    }\n",
        "\n",
        "    // Copy host matrices to device\n",
        "    cudaMemcpy(d_a, a, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Set up grid and block dimensions\n",
        "    dim3 gridDim(1, 1, 1);\n",
        "    dim3 blockDim(N, N, 1);\n",
        "\n",
        "    // Launch the matrixAdd kernel with specified grid and block dimensions\n",
        "    matrixAdd<<<gridDim, blockDim>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(c, d_c, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print result\n",
        "    printf(\"Matrix A:\\n\");\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            printf(\"%d\\t\", a[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nMatrix B:\\n\");\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            printf(\"%d\\t\", b[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nMatrix C (A + B):\\n\");\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            printf(\"%d\\t\", c[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVPx4clOK7YO"
      },
      "source": [
        "# **5) Matrix Transpose**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOO0wX-jKe33",
        "outputId": "8c6ddd6b-698e-4c02-e605-1931bbd4bcf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Matrix:\n",
            "0\t1\t2\t\n",
            "3\t4\t5\t\n",
            "6\t7\t8\t\n",
            "\n",
            "Transposed Matrix:\n",
            "0\t3\t6\t\n",
            "1\t4\t7\t\n",
            "2\t5\t8\t\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 3\n",
        "\n",
        "__global__ void matrixTranspose(int *a, int *c) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        c[col * N + row] = a[row * N + col];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int a[N][N], c[N][N];\n",
        "    int *d_a, *d_c;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&d_a, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_c, N * N * sizeof(int));\n",
        "\n",
        "    // Initialize host matrix\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        a[i / N][i % N] = i;\n",
        "    }\n",
        "\n",
        "    // Copy host matrix to device\n",
        "    cudaMemcpy(d_a, a, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Set up grid and block dimensions\n",
        "    dim3 gridDim(1, 1, 1);\n",
        "    dim3 blockDim(N, N, 1);\n",
        "\n",
        "    // Launch the matrixTranspose kernel with specified grid and block dimensions\n",
        "    matrixTranspose<<<gridDim, blockDim>>>(d_a, d_c);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(c, d_c, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print original matrix\n",
        "    printf(\"Original Matrix:\\n\");\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            printf(\"%d\\t\", a[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Print transposed matrix\n",
        "    printf(\"\\nTransposed Matrix:\\n\");\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            printf(\"%d\\t\", c[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJaG-lzbLDMc"
      },
      "source": [
        "# **6) Open MP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4TIEbT9K6ny"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "cat > codeopenmp.c <<EOF\n",
        "#include<stdio.h>\n",
        "#include<omp.h>\n",
        "\n",
        "int main() {\n",
        "    int thread_id;\n",
        "    #pragma omp parallel private(thread_id)\n",
        "    {\n",
        "        thread_id = omp_get_thread_num();\n",
        "        printf(\"Hello, world from thread %d \\n\", thread_id);\n",
        "    }\n",
        "    return 0;\n",
        "}\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnuL_WboLZHf",
        "outputId": "5a00e429-483b-4044-9303-f7f16334dfb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world from thread 0 \n",
            "Hello, world from thread 1 \n"
          ]
        }
      ],
      "source": [
        "!gcc -fopenmp codeopenmp.c && ./a.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rol1ib1VMNab"
      },
      "source": [
        "# **7) MPI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eAsblwJrMRdr"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "cat > mtc.c << EOF\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define MASTER_RANK 0\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size;\n",
        "    long long int total_points = 10000000000;\n",
        "    long long int points_inside_circle = 0;\n",
        "    long long int local_points_inside_circle = 0;\n",
        "    double x, y, distance;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    srand(time(NULL) + rank);\n",
        "\n",
        "    for (long long int i = 0; i < total_points / size; i++) {\n",
        "        x = (double)rand() / RAND_MAX;\n",
        "        y = (double)rand() / RAND_MAX;\n",
        "        distance = sqrt((x - 0.5) * (x - 0.5) + (y - 0.5) * (y - 0.5));\n",
        "        if (distance <= 0.5) {\n",
        "            local_points_inside_circle++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Reduce the local points inside the circle to reduce total count\n",
        "    MPI_Reduce(&local_points_inside_circle, &points_inside_circle, 1, MPI_LONG_LONG_INT, MPI_SUM, MASTER_RANK, MPI_COMM_WORLD);\n",
        "\n",
        "    // Below process calculates estimation of Pi value\n",
        "    if (rank == MASTER_RANK) {\n",
        "        double pi_estimate = 4.0 * points_inside_circle / total_points;\n",
        "        printf(\"Estimated value of Pi: %f\\n\", pi_estimate);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azmyCu6WMjtf",
        "outputId": "854be934-b03e-4d51-a6a4-9d130b496fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated value of Pi: 3.141590\n"
          ]
        }
      ],
      "source": [
        "!mpicc mtc.c -o mtc -lm && mpirun -n 4 --allow-run-as-root --oversubscribe ./mtc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uDk9Fl5MpYm"
      },
      "source": [
        "#**8) Monte Carlo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDkoTFPwMpHL",
        "outputId": "919b3f30-3cce-4ebd-d64f-6fd0493c6df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting simulation with 640 blocks, 32 threads, and 1000000 iterations\n",
            "Approximated PI using 20480000000 random tests\n",
            "PI ~= 3.1415979878906248\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <limits>\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "using std::cout;\n",
        "using std::endl;\n",
        "\n",
        "typedef unsigned long long Count;\n",
        "typedef std::numeric_limits<double> DblLim;\n",
        "\n",
        "const Count WARP_SIZE = 32;\n",
        "const Count NBLOCKS = 640;\n",
        "const Count ITERATIONS = 1000000;\n",
        "\n",
        "__global__ void picount(Count *totals) {\n",
        "    __shared__ Count counter[WARP_SIZE];\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    curandState_t rng;\n",
        "    curand_init(clock64(), tid, 0, &rng);\n",
        "    counter[threadIdx.x] = 0;\n",
        "    for (int i = 0; i < ITERATIONS; i++) {\n",
        "        float x = curand_uniform(&rng);\n",
        "        float y = curand_uniform(&rng);\n",
        "        counter[threadIdx.x] += 1 - int(x * x + y * y);\n",
        "    }\n",
        "    if (threadIdx.x == 0) {\n",
        "        totals[blockIdx.x] = 0;\n",
        "        for (int i = 0; i < WARP_SIZE; i++) {\n",
        "            totals[blockIdx.x] += counter[i];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    int numDev;\n",
        "    cudaGetDeviceCount(&numDev);\n",
        "    if (numDev < 1) {\n",
        "        cout << \"CUDA device missing! Do you need to use optirun?\\n\";\n",
        "        return 1;\n",
        "    }\n",
        "    cout << \"Starting simulation with \" << NBLOCKS << \" blocks, \" << WARP_SIZE << \" threads, and \" << ITERATIONS << \" iterations\\n\";\n",
        "\n",
        "    Count *hOut, *dOut;\n",
        "    hOut = new Count[NBLOCKS];\n",
        "    cudaMalloc(&dOut, sizeof(Count) * NBLOCKS);\n",
        "\n",
        "    picount<<<NBLOCKS, WARP_SIZE>>>(dOut);\n",
        "\n",
        "    cudaMemcpy(hOut, dOut, sizeof(Count) * NBLOCKS, cudaMemcpyDeviceToHost);\n",
        "    cudaFree(dOut);\n",
        "\n",
        "    Count total = 0;\n",
        "    for (int i = 0; i < NBLOCKS; i++) {\n",
        "        total += hOut[i];\n",
        "    }\n",
        "\n",
        "    Count tests = NBLOCKS * ITERATIONS * WARP_SIZE;\n",
        "    cout << \"Approximated PI using \" << tests << \" random tests\\n\";\n",
        "    cout.precision(DblLim::max_digits10);\n",
        "    cout << \"PI ~= \" << 4.0 * (double)total/(double)tests << endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVn4uvMlOAWB"
      },
      "source": [
        "# **9) FFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45BmXeUQLZ-K",
        "outputId": "5e2a3293-12f3-4211-c220-b19dd8e99579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFT Result in Matrix Form:\n",
            "(13.6569, -5.65685) (18.364, -6.36396) (17.4142, 8.58579) (22.1213, 8.87868) (-4.68629, 27.3137) (-2.85786, 29.7279) (7.89949, -30.3848) (10.1924, -35.9203) \n",
            "(-19.799, -19.799) (-20.9203, -22.3345) (63.6985, -24.9289) (72.234, -25.3934) (-2.82843, -2.82843) (-2.53553, -3.94975) (14, 0) (15, 0) \n",
            "(16, 0) (17, 0) (18, 0) (19, 0) (20, 0) (21, 0) (22, 0) (23, 0) \n",
            "(24, 0) (25, 0) (26, 0) (27, 0) (28, 0) (29, 0) (30, 0) (31, 0) \n",
            "(32, 0) (33, 0) (34, 0) (35, 0) (36, 0) (37, 0) (38, 0) (39, 0) \n",
            "(40, 0) (41, 0) (42, 0) (43, 0) (44, 0) (45, 0) (46, 0) (47, 0) \n",
            "(48, 0) (49, 0) (50, 0) (51, 0) (52, 0) (53, 0) (54, 0) (55, 0) \n",
            "(56, 0) (57, 0) (58, 0) (59, 0) (60, 0) (61, 0) (62, 0) (63, 0) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define PI 3.14159265358979323846\n",
        "\n",
        "// Complex number structure\n",
        "struct Complex {\n",
        "    float real;\n",
        "    float imag;\n",
        "};\n",
        "\n",
        "// Function to perform butterfly operation in FFT\n",
        "__device__ void butterfly(Complex& a, Complex& b, float cos_theta, float sin_theta) {\n",
        "    Complex temp;\n",
        "    temp.real = b.real * cos_theta - b.imag * sin_theta;\n",
        "    temp.imag = b.real * sin_theta + b.imag * cos_theta;\n",
        "    b.real = a.real - temp.real;\n",
        "    b.imag = a.imag - temp.imag;\n",
        "    a.real = a.real + temp.real;\n",
        "    a.imag = a.imag + temp.imag;\n",
        "}\n",
        "\n",
        "// Function to perform FFT on a single thread\n",
        "__global__ void fftKernel(Complex* data, int N) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    // Calculate offset and stride\n",
        "    int offset = tid % (N / 2);\n",
        "    int stride = N / 2;\n",
        "    float cos_theta, sin_theta;\n",
        "    for (int m = N; m >= 2; m /= 2) {\n",
        "        for (int k = 0; k < N / m; k++) {\n",
        "            int j = k * m + offset;\n",
        "            int i = j + stride;\n",
        "            cos_theta = cos(-2 * PI * k / N);\n",
        "            sin_theta = sin(-2 * PI * k / N);\n",
        "            butterfly(data[j], data[i], cos_theta, sin_theta);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to print complex numbers\n",
        "void printComplexMatrix(Complex* matrix, int rows, int cols) {\n",
        "    for (int i = 0; i < rows; ++i) {\n",
        "        for (int j = 0; j < cols; ++j) {\n",
        "            std::cout << \"(\" << matrix[i * cols + j].real << \", \" << matrix[i * cols + j].imag << \") \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 8; // Size of the input data (should be a power of 2 for this example)\n",
        "\n",
        "    // Allocate and initialize host data\n",
        "    Complex* h_data = new Complex[N * N];\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        h_data[i].real = static_cast<float>(i);\n",
        "        h_data[i].imag = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Allocate device data\n",
        "    Complex* d_data;\n",
        "    cudaMalloc((void**)&d_data, N * N * sizeof(Complex));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(d_data, h_data, N * N * sizeof(Complex), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch FFT kernel\n",
        "    dim3 blockSize(N);\n",
        "    dim3 gridSize(1);\n",
        "    fftKernel<<<gridSize, blockSize>>>(d_data, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(h_data, d_data, N * N * sizeof(Complex), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the result in matrix form\n",
        "    std::cout << \"FFT Result in Matrix Form:\" << std::endl;\n",
        "    printComplexMatrix(h_data, N, N);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_data);\n",
        "    delete[] h_data;\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWBbxv_kPdVG"
      },
      "source": [
        "# **10) N Queen**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpwviD2GLtU1",
        "outputId": "05c0cabd-6f1a-48c4-931a-a19ddfbe1a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solution 0:\n",
            "0 0 1 0 \n",
            "1 0 0 0 \n",
            "0 0 0 1 \n",
            "0 1 0 0 \n",
            "\n",
            "Solution 1:\n",
            "0 1 0 0 \n",
            "0 0 0 1 \n",
            "1 0 0 0 \n",
            "0 0 1 0 \n",
            "\n",
            "Number of solutions: 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 4\n",
        "\n",
        "__device__ bool isSafe(int board[N][N], int row, int col) {\n",
        "    int i, j;\n",
        "\n",
        "    // Check this row on left side\n",
        "    for (i = 0; i < col; i++)\n",
        "        if (board[row][i])\n",
        "            return false;\n",
        "\n",
        "    // Check upper diagonal on left side\n",
        "    for (i = row, j = col; i >= 0 && j >= 0; i--, j--)\n",
        "        if (board[i][j])\n",
        "            return false;\n",
        "\n",
        "    // Check lower diagonal on left side\n",
        "    for (i = row, j = col; j >= 0 && i < N; i++, j--)\n",
        "        if (board[i][j])\n",
        "            return false;\n",
        "\n",
        "    return true;\n",
        "}\n",
        "\n",
        "__device__ void solveNQueensKernel(int board[N][N], int col, int* count) {\n",
        "    // If all queens are placed then count this configuration\n",
        "    if (col == N) {\n",
        "        // Print the board\n",
        "        printf(\"Solution %d:\\n\", atomicAdd(count, 1));\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++)\n",
        "                printf(\"%d \", board[i][j]);\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Consider this column and try placing this queen in all rows one by one\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        // Check if the queen can be placed on board[i][col]\n",
        "        if (isSafe(board, i, col)) {\n",
        "            // Place this queen in board[i][col]\n",
        "            board[i][col] = 1;\n",
        "            // Recur to place rest of the queens\n",
        "            solveNQueensKernel(board, col + 1, count);\n",
        "            // Backtrack\n",
        "            board[i][col] = 0;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void solveNQueens(int* count) {\n",
        "    int board[N][N] = {{0}};\n",
        "    solveNQueensKernel(board, 0, count);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int* d_count;\n",
        "    cudaMalloc((void**)&d_count, sizeof(int));\n",
        "    int h_count = 0;\n",
        "    cudaMemcpy(d_count, &h_count, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    solveNQueens<<<1, 1>>>(d_count);\n",
        "\n",
        "    cudaMemcpy(&h_count, d_count, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    printf(\"Number of solutions: %d\\n\", h_count);\n",
        "\n",
        "    cudaFree(d_count);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhvaiYkpQBTf"
      },
      "source": [
        "# **11) Histogram Computation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rh55rmvP9sb",
        "outputId": "17430098-98c0-441a-ccfc-f84ed8fd637f"
      },
      "outputs": [],
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define NUM_BINS 256\n",
        "#define SIZE 1000\n",
        "\n",
        "__global__ void computeHistogram(const unsigned char *data, int size, int *histogram) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (tid < size) {\n",
        "        atomicAdd(&histogram[data[tid]], 1);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Host variables\n",
        "    unsigned char *h_data;\n",
        "    int *h_histogram;\n",
        "\n",
        "    // Device variables\n",
        "    unsigned char *d_data;\n",
        "    int *d_histogram;\n",
        "\n",
        "    // Allocate memory on host\n",
        "    h_data = (unsigned char *)malloc(SIZE * sizeof(unsigned char));\n",
        "    h_histogram = (int *)malloc(NUM_BINS * sizeof(int));\n",
        "\n",
        "    // Generate input data\n",
        "    for (int i = 0; i < SIZE; ++i) {\n",
        "        h_data[i] = rand() % NUM_BINS; // Random unsigned char values between 0 and 255\n",
        "    }\n",
        "\n",
        "    // Allocate memory on device\n",
        "    cudaMalloc((void **)&d_data, SIZE * sizeof(unsigned char));\n",
        "    cudaMalloc((void **)&d_histogram, NUM_BINS * sizeof(int));\n",
        "\n",
        "    // Copy input data from host to device\n",
        "    cudaMemcpy(d_data, h_data, SIZE * sizeof(unsigned char), cudaMemcpyHostToDevice);\n",
        "    cudaMemset(d_histogram, 0, NUM_BINS * sizeof(int)); // Initialize histogram on device\n",
        "\n",
        "    // Kernel configuration\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (SIZE + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Launch kernel\n",
        "    computeHistogram<<<numBlocks, blockSize>>>(d_data, SIZE, d_histogram);\n",
        "\n",
        "    // Copy histogram from device to host\n",
        "    cudaMemcpy(h_histogram, d_histogram, NUM_BINS * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print histogram\n",
        "    printf(\"Histogram:\\n\");\n",
        "    for (int i = 0; i < NUM_BINS; ++i) {\n",
        "        printf(\"%d: %d\\n\", i, h_histogram[i]);\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    free(h_data);\n",
        "    free(h_histogram);\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_histogram);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhGbItYrQw_C"
      },
      "source": [
        "# **12) Partical Simulation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzUPjamXQXjq",
        "outputId": "d230eb5e-a938-4390-c53a-af5bc1849726"
      },
      "outputs": [],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define NUM_PARTICLES 1000\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "typedef struct {\n",
        "    float x, y, z;\n",
        "    float vx, vy, vz;\n",
        "} Particle;\n",
        "\n",
        "void initializeParticles(Particle* particles, int num_particles) {\n",
        "    for (int i = 0; i < num_particles; i++) {\n",
        "        particles[i].x = (float)rand() / RAND_MAX;\n",
        "        particles[i].y = (float)rand() / RAND_MAX;\n",
        "        particles[i].z = (float)rand() / RAND_MAX;\n",
        "        particles[i].vx = 0.1 * ((float)rand() / RAND_MAX - 0.5);\n",
        "        particles[i].vy = 0.1 * ((float)rand() / RAND_MAX - 0.5);\n",
        "        particles[i].vz = 0.1 * ((float)rand() / RAND_MAX - 0.5);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void updateParticles(Particle* particles, int num_particles) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < num_particles) {\n",
        "        particles[idx].x += particles[idx].vx;\n",
        "        particles[idx].y += particles[idx].vy;\n",
        "        particles[idx].z += particles[idx].vz;\n",
        "        particles[idx].vx += 0.01;\n",
        "        particles[idx].vy += 0.01;\n",
        "        particles[idx].vz += 0.01;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    Particle* particles, *d_particles;\n",
        "    particles = (Particle*)malloc(NUM_PARTICLES * sizeof(Particle));\n",
        "    cudaMalloc(&d_particles, NUM_PARTICLES * sizeof(Particle));\n",
        "\n",
        "    initializeParticles(particles, NUM_PARTICLES);\n",
        "    cudaMemcpy(d_particles, particles, NUM_PARTICLES * sizeof(Particle), cudaMemcpyHostToDevice);\n",
        "\n",
        "    printf(\"Initial particle positions:\\n\");\n",
        "    for (int i = 0; i < NUM_PARTICLES; i++) {\n",
        "        printf(\"Particle %d: (%f, %f, %f)\\n\", i, particles[i].x, particles[i].y, particles[i].z);\n",
        "    }\n",
        "\n",
        "    updateParticles<<<(NUM_PARTICLES + BLOCK_SIZE - 1) / BLOCK_SIZE, BLOCK_SIZE>>>(d_particles, NUM_PARTICLES);\n",
        "\n",
        "    cudaMemcpy(particles, d_particles, NUM_PARTICLES * sizeof(Particle), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nUpdated particle positions:\\n\");\n",
        "    for (int i = 0; i < NUM_PARTICLES; i++) {\n",
        "        printf(\"Particle %d: (%f, %f, %f)\\n\", i, particles[i].x, particles[i].y, particles[i].z);\n",
        "    }\n",
        "\n",
        "    free(particles);\n",
        "    cudaFree(d_particles);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyCI2CaXpx04"
      },
      "source": [
        "# **13) Merge Sort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7xmXw4iRadz",
        "outputId": "a1fc319e-8ba3-443d-9a27-e91f652c1be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original array:\n",
            "41 67 34 0 69 24 78 58 62 64 \n",
            "Sorted array:\n",
            "0 34 34 41 67 69 69 78 64 78 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 256\n",
        "\n",
        "__device__ void merge(int* arr, int l, int m, int r) {\n",
        "    int n1 = m - l + 1;\n",
        "    int n2 = r - m;\n",
        "\n",
        "    int* L = (int*)malloc(n1 * sizeof(int));\n",
        "    int* R = (int*)malloc(n2 * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n1; i++)\n",
        "        L[i] = arr[l + i];\n",
        "    for (int j = 0; j < n2; j++)\n",
        "        R[j] = arr[m + 1 + j];\n",
        "\n",
        "    int i = 0, j = 0, k = l;\n",
        "    while (i < n1 && j < n2) {\n",
        "        if (L[i] <= R[j]) {\n",
        "            arr[k] = L[i];\n",
        "            i++;\n",
        "        } else {\n",
        "            arr[k] = R[j];\n",
        "            j++;\n",
        "        }\n",
        "        k++;\n",
        "    }\n",
        "\n",
        "    while (i < n1) {\n",
        "        arr[k] = L[i];\n",
        "        i++;\n",
        "        k++;\n",
        "    }\n",
        "\n",
        "    while (j < n2) {\n",
        "        arr[k] = R[j];\n",
        "        j++;\n",
        "        k++;\n",
        "    }\n",
        "\n",
        "    free(L);\n",
        "    free(R);\n",
        "}\n",
        "\n",
        "__global__ void mergeSortKernel(int* arr, int n) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int blockSize = blockDim.x;\n",
        "    int start = tid * n / (blockSize * gridDim.x);\n",
        "    int end = (tid + 1) * n / (blockSize * gridDim.x) - 1;\n",
        "\n",
        "    for (int size = 2; size <= n; size *= 2) {\n",
        "        for (int subSize = size / 2; subSize > 0; subSize /= 2) {\n",
        "            __syncthreads();\n",
        "\n",
        "            int step = size / subSize;\n",
        "            int k = tid / step * step;\n",
        "\n",
        "            if (k < n && k + subSize < n && k / blockSize == (k + subSize) / blockSize) {\n",
        "                merge(arr, k, k + subSize - 1, k + size - 1);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void mergeSort(int* arr, int n) {\n",
        "    int* d_arr;\n",
        "    cudaMalloc(&d_arr, n * sizeof(int));\n",
        "    cudaMemcpy(d_arr, arr, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blocks = (n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "    mergeSortKernel<<<blocks, THREADS_PER_BLOCK>>>(d_arr, n);\n",
        "\n",
        "    cudaMemcpy(arr, d_arr, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_arr);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 10;\n",
        "    int* arr = (int*)malloc(n * sizeof(int));\n",
        "\n",
        "    printf(\"Original array:\\n\");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        arr[i] = rand() % 100;\n",
        "        printf(\"%d \", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    mergeSort(arr, n);\n",
        "\n",
        "    printf(\"Sorted array:\\n\");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%d \", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    free(arr);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqtzHn3nuDED"
      },
      "source": [
        "# **14) Quick Sort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKgDiDtrp3Od",
        "outputId": "5ee7fed7-a802-4e13-9f92-3941f3c45c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorted array: 1 5 6 7 10 11 12 13 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "__device__ void swap(int &a, int &b) {\n",
        "    int temp = a;\n",
        "    a = b;\n",
        "    b = temp;\n",
        "}\n",
        "\n",
        "__device__ int partition(int *arr, int low, int high) {\n",
        "    int pivot = arr[high];\n",
        "    int i = low - 1;\n",
        "    for (int j = low; j < high; j++) {\n",
        "        if (arr[j] < pivot) {\n",
        "            i++;\n",
        "            swap(arr[i], arr[j]);\n",
        "        }\n",
        "    }\n",
        "    swap(arr[i + 1], arr[high]);\n",
        "    return i + 1;\n",
        "}\n",
        "\n",
        "// Recursive quickSort function\n",
        "__device__ void quickSortRecursive(int *arr, int low, int high) {\n",
        "    if (low < high) {\n",
        "        int pi = partition(arr, low, high);\n",
        "        quickSortRecursive(arr, low, pi - 1);\n",
        "        quickSortRecursive(arr, pi + 1, high);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void quickSort(int *arr, int low, int high) {\n",
        "    if (low < high) {\n",
        "        int pi = partition(arr, low, high);\n",
        "        // Call the recursive function\n",
        "        quickSortRecursive(arr, low, pi - 1);\n",
        "        quickSortRecursive(arr, pi + 1, high);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 8;\n",
        "    int h_arr[size] = {12, 11, 13, 5, 6, 7, 1, 10};\n",
        "    int *d_arr;\n",
        "    cudaMalloc((void**)&d_arr, size * sizeof(int));\n",
        "    cudaMemcpy(d_arr, h_arr, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    quickSort<<<1, 1>>>(d_arr, 0, size - 1);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(h_arr, d_arr, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    std::cout << \"Sorted array: \";\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        std::cout << h_arr[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "    cudaFree(d_arr);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63wN5w30U6b"
      },
      "source": [
        "# **15) Radix Sort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcmKyK3cuGWx",
        "outputId": "e88b4bc4-1ba9-49af-d806-26895bda7d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorted array: 2 24 45 66 75 90 170 802 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__device__ int getMax(int arr[], int n) {\n",
        "    int max = arr[0];\n",
        "    for (int i = 1; i < n; i++) {\n",
        "        if (arr[i] > max)\n",
        "            max = arr[i];\n",
        "    }\n",
        "    return max;\n",
        "}\n",
        "\n",
        "__device__ void countSort(int arr[], int n, int exp) {\n",
        "    int *output = new int[n];\n",
        "    int count[10] = {0};\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "        count[(arr[i] / exp) % 10]++;\n",
        "\n",
        "    for (int i = 1; i < 10; i++)\n",
        "        count[i] += count[i - 1];\n",
        "\n",
        "    for (int i = n - 1; i >= 0; i--) {\n",
        "        output[count[(arr[i] / exp) % 10] - 1] = arr[i];\n",
        "        count[(arr[i] / exp) % 10]--;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n; i++)\n",
        "        arr[i] = output[i];\n",
        "\n",
        "    delete[] output;\n",
        "}\n",
        "\n",
        "__global__ void radixSort(int arr[], int n) {\n",
        "    int max = getMax(arr, n);\n",
        "\n",
        "    for (int exp = 1; max / exp > 0; exp *= 10)\n",
        "        countSort(arr, n, exp);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 8;\n",
        "    int h_arr[size] = {170, 45, 75, 90, 802, 24, 2, 66};\n",
        "    int *d_arr;\n",
        "    cudaMalloc((void**)&d_arr, size * sizeof(int));\n",
        "    cudaMemcpy(d_arr, h_arr, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    radixSort<<<1, 1>>>(d_arr, size);\n",
        "    cudaMemcpy(h_arr, d_arr, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    std::cout << \"Sorted array: \";\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        std::cout << h_arr[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "    cudaFree(d_arr);\n",
        "    return 0;\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
